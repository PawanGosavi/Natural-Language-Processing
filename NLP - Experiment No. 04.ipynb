{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K8mpiDNwAfU"
      },
      "source": [
        "# **Exp No.4 : Write a Program to generate N-Gram of English and Hindi Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CAMm0JKB1PW"
      },
      "source": [
        "# **Name: Pawan Gosavi MSc IT Part II 6909**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXGVtQ3UXrSj"
      },
      "source": [
        "**What Are N-Grams?**<br>\n",
        "N-Grams are words, or combinations of words, broken out by the number of words in that combination. As an outline:<br>\n",
        "\n",
        "Unigrams: one word<br>\n",
        "Bigrams: two words<br>\n",
        "Trigrams: three words<br>\n",
        "And so forth<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyhdaH4mXuDt"
      },
      "source": [
        "To further explore n-grams, we can break down the sentence below: <br>\n",
        "Hi there everyone, we’re exploring n-grams today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ldNhoWcitv"
      },
      "source": [
        "Unigram: hi | there | everyone, etc… <br>\n",
        "Bigram: hi there | exploring n-grams | etc…  <br>\n",
        "Trigram: hi there everyone | exploring n-grams today | etc…  <br>\n",
        "Note that the words must follow sequentially to be an n-gram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQvtRLehd9RH"
      },
      "source": [
        "*    Imagine listening to someone as they speak and trying to guess the next word that they are going to say. For example what word is likely to follow this sentence fragment?:\n",
        "I’d like to make a . . .     / Please hand over your\n",
        "*   Guessing the next word (or word prediction) is an essential subtask of speech recognition, hand-writing recognition, augmentative communication for the disabled, and spelling error detection.\n",
        "*   In such tasks, word-identification is difficult because the input is very noisy and ambiguous.\n",
        "*   Thus looking at previous words can give us an important cue about what the next ones are going to be.\n",
        "*  N-gram models, which predict the next word from the previous N − 1 words.\n",
        "*   Such statistical models of word sequences are also called language models or LMs.\n",
        "*   Computing the probability of the next word will turn out to be closely related to computing the probability of a sequence of words.\n",
        "*   The following sequence, for example, has a non-zero probability of appearing in a text: \n",
        ". . . all of a sudden I notice three guys standing on the sidewalk... \" be off close on for joker\"\n",
        "*   while this same set of words in a different order has a much much lower probability: \n",
        "on guys all I of notice sidewalk three a sudden standing the\n",
        "*  It can also help to make spelling error corrections.\n",
        "*  For instance, the sentence “drink cofee” could be corrected to “drink coffee” if you knew that the word “coffee” had a high probability of occurrence after the word “drink” and also the overlap of letters between “cofee” and “coffee” is high\n",
        "*  Let’s start with equation P(w|h), the probability of word w, given some history, h. For example, P(The|its water is so transparent that)\n",
        "Here,<br>\n",
        "w = The <br>\n",
        "h = its water is so transparent that\n",
        "*  And, one way to estimate the above probability function is through the relative frequency count approach, where you would take a substantially large corpus, count the number of times you see *its water is so transparent that*, and then count the number of times it is followed by *the*. \n",
        "*  In other words, you are answering the question:\n",
        "Out of the times you saw the history h, how many times did the word w follow it\n",
        "P(the|its water is so transparent that) = C(its water is so transparent that)/C(its water is so transparent that the)\n",
        "*   You can imagine, it is not feasible to perform this over an entire corpus; especially if it is of a significant size.\n",
        "*   This shortcoming and ways to decompose the probability function using the chain rule serves as the base intuition of the N-gram model. Here, you, instead of computing probability using the entire corpus, would approximate it by just a few historical words\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANF71ZDQx1z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b01c82-278c-45df-c4b5-3375a65bd1ec"
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlwyBnjfJaQV",
        "outputId": "8209c351-d4ba-467c-93c9-404373321e31"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DFgPWLLclzQ",
        "outputId": "3ac91937-d974-4922-a394-0b81c986e7ca"
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        " \n",
        "# Function to generate n-grams from English Sentences.\n",
        "\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]    \n",
        "  \n",
        "English_Sentence = 'Three Things to Remember! Hustle, Loyalty, Respect!'\n",
        "\n",
        "n = len(English_Sentence.split())\n",
        " \n",
        "print(\"N Grams in English\\n\")\n",
        "\n",
        "for i in range(n):\n",
        "  print(i, \"- gram: \", extract_ngrams(English_Sentence, i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N Grams in English\n",
            "\n",
            "0 - gram:  ['Three', 'Things', 'to', 'Remember', '!', 'Hustle', ',', 'Loyalty', ',', 'Respect', '!']\n",
            "1 - gram:  ['Three', 'Things', 'to', 'Remember', '!', 'Hustle', ',', 'Loyalty', ',', 'Respect', '!']\n",
            "2 - gram:  ['Three Things', 'Things to', 'to Remember', 'Remember !', '! Hustle', 'Hustle ,', ', Loyalty', 'Loyalty ,', ', Respect', 'Respect !']\n",
            "3 - gram:  ['Three Things to', 'Things to Remember', 'to Remember !', 'Remember ! Hustle', '! Hustle ,', 'Hustle , Loyalty', ', Loyalty ,', 'Loyalty , Respect', ', Respect !']\n",
            "4 - gram:  ['Three Things to Remember', 'Things to Remember !', 'to Remember ! Hustle', 'Remember ! Hustle ,', '! Hustle , Loyalty', 'Hustle , Loyalty ,', ', Loyalty , Respect', 'Loyalty , Respect !']\n",
            "5 - gram:  ['Three Things to Remember !', 'Things to Remember ! Hustle', 'to Remember ! Hustle ,', 'Remember ! Hustle , Loyalty', '! Hustle , Loyalty ,', 'Hustle , Loyalty , Respect', ', Loyalty , Respect !']\n",
            "6 - gram:  ['Three Things to Remember ! Hustle', 'Things to Remember ! Hustle ,', 'to Remember ! Hustle , Loyalty', 'Remember ! Hustle , Loyalty ,', '! Hustle , Loyalty , Respect', 'Hustle , Loyalty , Respect !']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2oNr8qq-UDW",
        "outputId": "a72e9ec3-3c5b-4b07-b562-0ecf8212a707"
      },
      "source": [
        "# Function to generate n-grams from Hindi Sentences.\n",
        "\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]    \n",
        "  \n",
        "Hindi_Sentence = 'तीन चीज़ें याद रखें! ऊधम, निष्ठा, सम्मान!'\n",
        "\n",
        "n = len(Hindi_Sentence.split())\n",
        " \n",
        "print(\"N Grams in Hindi\\n\")\n",
        "\n",
        "for i in range(n):\n",
        "  print(i, \"- gram: \", extract_ngrams(Hindi_Sentence, i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N Grams in Hindi\n",
            "\n",
            "0 - gram:  ['तीन', 'चीज़ें', 'याद', 'रखें', '!', 'ऊधम', ',', 'निष्ठा', ',', 'सम्मान', '!']\n",
            "1 - gram:  ['तीन', 'चीज़ें', 'याद', 'रखें', '!', 'ऊधम', ',', 'निष्ठा', ',', 'सम्मान', '!']\n",
            "2 - gram:  ['तीन चीज़ें', 'चीज़ें याद', 'याद रखें', 'रखें !', '! ऊधम', 'ऊधम ,', ', निष्ठा', 'निष्ठा ,', ', सम्मान', 'सम्मान !']\n",
            "3 - gram:  ['तीन चीज़ें याद', 'चीज़ें याद रखें', 'याद रखें !', 'रखें ! ऊधम', '! ऊधम ,', 'ऊधम , निष्ठा', ', निष्ठा ,', 'निष्ठा , सम्मान', ', सम्मान !']\n",
            "4 - gram:  ['तीन चीज़ें याद रखें', 'चीज़ें याद रखें !', 'याद रखें ! ऊधम', 'रखें ! ऊधम ,', '! ऊधम , निष्ठा', 'ऊधम , निष्ठा ,', ', निष्ठा , सम्मान', 'निष्ठा , सम्मान !']\n",
            "5 - gram:  ['तीन चीज़ें याद रखें !', 'चीज़ें याद रखें ! ऊधम', 'याद रखें ! ऊधम ,', 'रखें ! ऊधम , निष्ठा', '! ऊधम , निष्ठा ,', 'ऊधम , निष्ठा , सम्मान', ', निष्ठा , सम्मान !']\n",
            "6 - gram:  ['तीन चीज़ें याद रखें ! ऊधम', 'चीज़ें याद रखें ! ऊधम ,', 'याद रखें ! ऊधम , निष्ठा', 'रखें ! ऊधम , निष्ठा ,', '! ऊधम , निष्ठा , सम्मान', 'ऊधम , निष्ठा , सम्मान !']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crp9ejTsIlQP",
        "outputId": "86d50279-8ed3-49f8-fc92-6a1f7114ba2f"
      },
      "source": [
        "# Function to generate n-grams from Marathi Sentences.\n",
        "\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]    \n",
        "  \n",
        "Marathi_Sentence = 'तीन गोष्टी लक्षात ठेवा! उधळपट्टी, निष्ठा, आदर!'\n",
        "\n",
        "n = len(Marathi_Sentence.split())\n",
        " \n",
        "print(\"N Grams in Marathi\\n\")\n",
        "\n",
        "for i in range(n):\n",
        "  print(i, \"- gram: \", extract_ngrams(Marathi_Sentence, i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N Grams in Marathi\n",
            "\n",
            "0 - gram:  ['तीन', 'गोष्टी', 'लक्षात', 'ठेवा', '!', 'उधळपट्टी', ',', 'निष्ठा', ',', 'आदर', '!']\n",
            "1 - gram:  ['तीन', 'गोष्टी', 'लक्षात', 'ठेवा', '!', 'उधळपट्टी', ',', 'निष्ठा', ',', 'आदर', '!']\n",
            "2 - gram:  ['तीन गोष्टी', 'गोष्टी लक्षात', 'लक्षात ठेवा', 'ठेवा !', '! उधळपट्टी', 'उधळपट्टी ,', ', निष्ठा', 'निष्ठा ,', ', आदर', 'आदर !']\n",
            "3 - gram:  ['तीन गोष्टी लक्षात', 'गोष्टी लक्षात ठेवा', 'लक्षात ठेवा !', 'ठेवा ! उधळपट्टी', '! उधळपट्टी ,', 'उधळपट्टी , निष्ठा', ', निष्ठा ,', 'निष्ठा , आदर', ', आदर !']\n",
            "4 - gram:  ['तीन गोष्टी लक्षात ठेवा', 'गोष्टी लक्षात ठेवा !', 'लक्षात ठेवा ! उधळपट्टी', 'ठेवा ! उधळपट्टी ,', '! उधळपट्टी , निष्ठा', 'उधळपट्टी , निष्ठा ,', ', निष्ठा , आदर', 'निष्ठा , आदर !']\n",
            "5 - gram:  ['तीन गोष्टी लक्षात ठेवा !', 'गोष्टी लक्षात ठेवा ! उधळपट्टी', 'लक्षात ठेवा ! उधळपट्टी ,', 'ठेवा ! उधळपट्टी , निष्ठा', '! उधळपट्टी , निष्ठा ,', 'उधळपट्टी , निष्ठा , आदर', ', निष्ठा , आदर !']\n",
            "6 - gram:  ['तीन गोष्टी लक्षात ठेवा ! उधळपट्टी', 'गोष्टी लक्षात ठेवा ! उधळपट्टी ,', 'लक्षात ठेवा ! उधळपट्टी , निष्ठा', 'ठेवा ! उधळपट्टी , निष्ठा ,', '! उधळपट्टी , निष्ठा , आदर', 'उधळपट्टी , निष्ठा , आदर !']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvRRw_cJCDkE"
      },
      "source": [
        "What is a Language Model in NLP?\n",
        "\n",
        "A language model learns to predict the probability of a sequence of words. But why do we need to learn the probability of words? Let’s understand that with an example.\n",
        "\n",
        "One of the use of language model is in  Machine Translation, you take in a bunch of words from a language and convert these words into another language. Now, there can be many potential translations that a system might give you and you will want to compute the probability of each of these translations to understand which one is the most accurate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zCfFoKiDWRk"
      },
      "source": [
        "In the above example, we know that the probability of the first sentence will be more than the second, right? That’s how we arrive at the right translation.\n",
        "\n",
        "This ability to model the rules of a language as a probability gives great power for NLP related tasks. Language models are used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval, and many other daily tasks.\n",
        "\n",
        "There are primarily two types of Language Models:\n",
        "\n",
        "*  Statistical Language Models: These models use traditional statistical techniques like N-grams, Hidden Markov Models (HMM) and certain linguistic rules to learn the probability distribution of words\n",
        "*  Neural Language Models: These are new players in the NLP town and have surpassed the statistical language models in their effectiveness. They use different kinds of Neural Networks to model language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLfZpjTiDjVj"
      },
      "source": [
        "How do N-gram Language Models work?\n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n",
        "\n",
        "An N-gram language model predicts the probability of a given N-gram within any sequence of words in the language. If we have a good N-gram model, we can predict p(w | h) – what is the probability of seeing the word w given a history of previous words h – where the history contains n-1 words.\n",
        "\n",
        "We must estimate this probability to construct an N-gram model.\n",
        "\n",
        "We compute this probability in two steps:\n",
        "\n",
        "1.   Apply the chain rule of probability\n",
        "2.   We then apply a very strong simplification assumption to allow us to compute p(w1…ws) in an easy manner\n",
        "\n",
        "\n",
        "The chain rule of probability is:\n",
        "\n",
        "p(w1...ws) = p(w1) . p(w2 | w1) . p(w3 | w1 w2) . p(w4 | w1 w2 w3) ..... p(wn | w1...wn-1)\n",
        "\n",
        "So what is the chain rule? It tells us how to compute the joint probability of a sequence by using the conditional probability of a word given previous words.\n",
        "\n",
        "But we do not have access to these conditional probabilities with complex conditions of up to n-1 words. So how do we proceed?\n",
        "\n",
        "This is where we introduce a simplification assumption. We can assume for all conditions, that:\n",
        "\n",
        "p(wk | w1...wk-1) = p(wk | wk-1)\n",
        "\n",
        "Here, we approximate the history (the context) of the word wk by looking only at the last word of the context. This assumption is called the Markov assumption. (We used it here with a simplified context of length 1 – which corresponds to a bigram model – we could use larger fixed-sized histories in general).\n",
        "\n",
        " \n",
        "Building a Basic Language Model\n",
        "Now that we understand what an N-gram is, let’s build a basic language model using trigrams of the Reuters corpus. Reuters corpus is a collection of 10,788 news documents totaling 1.3 million words. We can build a language model in a few lines of code using the NLTK package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TOdTykQmyqk",
        "outputId": "4aa45cb9-a0f6-493e-c2c6-5b9298e20822"
      },
      "source": [
        "from nltk.corpus import reuters\n",
        "\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "\n",
        "#!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAO3vUg4I6qb"
      },
      "source": [
        "# Text Generation using N Gram for English Language\n",
        "\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Create a placeholder for English Model\n",
        "English_Model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in reuters.sents():\n",
        "  for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "    English_Model[(w1, w2)][w3] += 1\n",
        " \n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in English_Model:\n",
        "  total_count = float(sum(English_Model[w1_w2].values()))\n",
        "  for w3 in English_Model[w1_w2]:\n",
        "    English_Model[w1_w2][w3] /= total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHdHg1DoIjDj"
      },
      "source": [
        "In the above We first split our text into trigrams with the help of NLTK and then calculate the frequency in which each combination of the trigrams occurs in the dataset.\n",
        "\n",
        "We then use it to calculate probabilities of a word, given the previous two words. That’s essentially what gives us our Language Model!\n",
        "\n",
        "Let’s make simple predictions with this language model. We will start with two simple words – “today the”. We want our model to tell us what will be the next word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlpRYhwFIqir",
        "outputId": "685d519a-e9bb-4dfa-d2e6-5e11684cda85"
      },
      "source": [
        "# N Gram fro English Language\n",
        "\n",
        "English_Model['today' , 'the']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>.<locals>.<lambda>>,\n",
              "            {'Bank': 0.05555555555555555,\n",
              "             'European': 0.05555555555555555,\n",
              "             'Higher': 0.05555555555555555,\n",
              "             'Italian': 0.05555555555555555,\n",
              "             'Turkish': 0.05555555555555555,\n",
              "             'company': 0.16666666666666666,\n",
              "             'emirate': 0.05555555555555555,\n",
              "             'increase': 0.05555555555555555,\n",
              "             'newspaper': 0.05555555555555555,\n",
              "             'options': 0.05555555555555555,\n",
              "             'overseas': 0.05555555555555555,\n",
              "             'pound': 0.05555555555555555,\n",
              "             'price': 0.1111111111111111,\n",
              "             'public': 0.05555555555555555,\n",
              "             'time': 0.05555555555555555})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwxPJCoWIp3r"
      },
      "source": [
        "So we get predictions of all the possible words that can come next with their respective probabilities. Now, if we pick up the word “price” and again make a prediction for the words “the” and “price”:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdNrpL0sIvCc",
        "outputId": "5dbf26df-be70-4a33-cdb9-b2062afee2e4"
      },
      "source": [
        "dict(English_Model[\"the\",\"price\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 0.009302325581395349,\n",
              " ',': 0.018604651162790697,\n",
              " ',\"': 0.004651162790697674,\n",
              " '-': 0.004651162790697674,\n",
              " '.': 0.023255813953488372,\n",
              " 'Royal': 0.004651162790697674,\n",
              " 'action': 0.004651162790697674,\n",
              " 'adjustment': 0.023255813953488372,\n",
              " 'adjustments': 0.004651162790697674,\n",
              " 'again': 0.004651162790697674,\n",
              " 'and': 0.004651162790697674,\n",
              " 'approached': 0.004651162790697674,\n",
              " 'at': 0.023255813953488372,\n",
              " 'base': 0.004651162790697674,\n",
              " 'being': 0.004651162790697674,\n",
              " 'changes': 0.004651162790697674,\n",
              " 'climate': 0.004651162790697674,\n",
              " 'collapse': 0.004651162790697674,\n",
              " 'could': 0.004651162790697674,\n",
              " 'cut': 0.009302325581395349,\n",
              " 'cuts': 0.009302325581395349,\n",
              " 'difference': 0.004651162790697674,\n",
              " 'differentials': 0.009302325581395349,\n",
              " 'drop': 0.004651162790697674,\n",
              " 'effect': 0.004651162790697674,\n",
              " 'evolution': 0.004651162790697674,\n",
              " 'factor': 0.004651162790697674,\n",
              " 'fall': 0.004651162790697674,\n",
              " 'for': 0.05116279069767442,\n",
              " 'freeze': 0.009302325581395349,\n",
              " 'from': 0.004651162790697674,\n",
              " 'gap': 0.004651162790697674,\n",
              " 'guaranteed': 0.004651162790697674,\n",
              " 'had': 0.004651162790697674,\n",
              " 'has': 0.009302325581395349,\n",
              " 'hike': 0.004651162790697674,\n",
              " 'holds': 0.004651162790697674,\n",
              " 'in': 0.004651162790697674,\n",
              " 'increase': 0.009302325581395349,\n",
              " 'increases': 0.013953488372093023,\n",
              " 'instability': 0.004651162790697674,\n",
              " 'is': 0.018604651162790697,\n",
              " 'it': 0.05581395348837209,\n",
              " 'led': 0.004651162790697674,\n",
              " 'limit': 0.004651162790697674,\n",
              " 'move': 0.004651162790697674,\n",
              " 'moved': 0.004651162790697674,\n",
              " 'now': 0.004651162790697674,\n",
              " 'of': 0.3209302325581395,\n",
              " 'on': 0.004651162790697674,\n",
              " 'outlook': 0.004651162790697674,\n",
              " 'paid': 0.013953488372093023,\n",
              " 'per': 0.013953488372093023,\n",
              " 'policy': 0.004651162790697674,\n",
              " 'projected': 0.004651162790697674,\n",
              " 'raise': 0.004651162790697674,\n",
              " 'reductions': 0.004651162790697674,\n",
              " 'related': 0.004651162790697674,\n",
              " 'review': 0.004651162790697674,\n",
              " 'revision': 0.004651162790697674,\n",
              " 'rises': 0.004651162790697674,\n",
              " 'slump': 0.004651162790697674,\n",
              " 'slumped': 0.004651162790697674,\n",
              " 'stayed': 0.009302325581395349,\n",
              " 'structure': 0.004651162790697674,\n",
              " 'support': 0.004651162790697674,\n",
              " 'the': 0.013953488372093023,\n",
              " 'to': 0.05581395348837209,\n",
              " 'used': 0.004651162790697674,\n",
              " 'was': 0.009302325581395349,\n",
              " 'we': 0.004651162790697674,\n",
              " 'went': 0.004651162790697674,\n",
              " 'while': 0.004651162790697674,\n",
              " 'will': 0.013953488372093023,\n",
              " 'would': 0.009302325581395349,\n",
              " 'yesterday': 0.004651162790697674,\n",
              " 'zone': 0.004651162790697674}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auTKvizsIuc8"
      },
      "source": [
        "If we keep following this process iteratively, we will soon have a coherent sentence! Here is a script to play around with generating a random piece of text using our n-gram model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exNv3ngMIyj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef6a594-e55b-4a14-9aa2-9996a423f3f5"
      },
      "source": [
        "import random\n",
        "\n",
        "# starting words\n",
        "E_text = ['today', 'the']\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in English_Model[tuple(E_text[-2:])].keys():\n",
        "      accumulator += English_Model[tuple(E_text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          E_text.append(word)\n",
        "          break\n",
        "\n",
        "  if E_text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in E_text if t]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today the emirate ' s unstable political situation ,\" he said .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PMJeIoy90H7"
      },
      "source": [
        "# Text Generation using N Gram for English Language\n",
        "\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Create a placeholder for Hindi Model\n",
        "Hindi_Model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "Hindi_Text = {'मैं कौन हूँ और मेरा नाम क्या हैं , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं कहा से हूँ , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं क्या करता हूँ , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं अभी कहा पर हूँ , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं यहाँ क्या करने आया हूँ , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं तुम्हें पहलें से जानता हूँ कि नहीं , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं किस से मिलने वाला हूँ , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं कही किसी भ्रम में तो नहीं , यह भी मुझे नहीं पता है ।', \n",
        "              'मैं आखिर कुछ जानता हूँ भी या नहीं , यह भी मुझे नहीं पता है ।',\n",
        "              }\n",
        "\n",
        "Hindi_Corpus = []\n",
        "\n",
        "for text in Hindi_Text:\n",
        "  word_tokens_hindi = word_tokenize(text) \n",
        "  tokenized_sentence_hindi = []  \n",
        "  for w in word_tokens_hindi:\n",
        "      tokenized_sentence_hindi.append(w)\n",
        "  # print(tokenized_sentence_hindi)\n",
        "  Hindi_Corpus.append(tokenized_sentence_hindi)\n",
        "\n",
        "#for s in Hindi_Corpus:\n",
        "  #print(s)\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in Hindi_Corpus:\n",
        "  for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "    Hindi_Model[(w1, w2)][w3] += 1\n",
        " \n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in Hindi_Model:\n",
        "  total_count = float(sum(Hindi_Model[w1_w2].values()))\n",
        "  for w3 in Hindi_Model[w1_w2]:\n",
        "    Hindi_Model[w1_w2][w3] /= total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3yuPqqckNs",
        "outputId": "9533c28e-f1b1-46c6-da96-4fc24bd9a700"
      },
      "source": [
        "# N Gram fro Hindi Language\n",
        "\n",
        "Hindi_Model['यह', 'भी']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>.<locals>.<lambda>>, {'मुझे': 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTI8JkPGds4O",
        "outputId": "f096f5c1-f6a1-4e8c-fb44-c69055e582f7"
      },
      "source": [
        "dict(Hindi_Model['यह', 'भी'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'मुझे': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niKL6qc0d7ED",
        "outputId": "bca5eab9-9ea1-4fd7-df7b-1c705049b7a0"
      },
      "source": [
        "import random\n",
        "\n",
        "# starting words\n",
        "H_text = ['यह', 'भी']\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in Hindi_Model[tuple(H_text[-2:])].keys():\n",
        "      accumulator += Hindi_Model[tuple(H_text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          H_text.append(word)\n",
        "          break\n",
        "\n",
        "  if H_text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in H_text if t]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "यह भी मुझे नहीं पता है ।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W2fLi3cMYfc"
      },
      "source": [
        "Limitations of N-gram approach to Language Modeling\n",
        "N-gram based language models do have a few drawbacks:\n",
        "\n",
        "The higher the N, the better is the model usually. But this leads to lots of computation overhead that requires large computation power in terms of RAM\n",
        "N-grams are a sparse representation of language. This is because we build the model based on the probability of words co-occurring. It will give zero probability to all the words that are not present in the training corpus\n",
        " "
      ]
    }
  ]
}
